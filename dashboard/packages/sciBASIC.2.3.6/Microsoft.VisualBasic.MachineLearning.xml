<?xml version="1.0"?>
<doc>
<assembly>
<name>
Microsoft.VisualBasic.MachineLearning
</name>
</assembly>
<members>
<member name="T:Microsoft.VisualBasic.MachineLearning.My.Resources.Resources">
<summary>
  A strongly-typed resource class, for looking up localized strings, etc.
</summary>
</member>
<member name="P:Microsoft.VisualBasic.MachineLearning.My.Resources.Resources.ResourceManager">
<summary>
  Returns the cached ResourceManager instance used by this class.
</summary>
</member>
<member name="P:Microsoft.VisualBasic.MachineLearning.My.Resources.Resources.Culture">
<summary>
  Overrides the current thread's CurrentUICulture property for all
  resource lookups using this strongly typed resource class.
</summary>
</member>
<member name="T:Microsoft.VisualBasic.MachineLearning.Darwinism.DifferentialEvolution">
 <summary>
 In evolutionary computation, differential evolution (DE) is a method that optimizes a problem by 
 iteratively trying to improve a candidate solution with regard to a given measure of quality. 
 Such methods are commonly known as metaheuristics as they make few or no assumptions about the 
 problem being optimized and can search very large spaces of candidate solutions. However, 
 metaheuristics such as DE do not guarantee an optimal solution is ever found.
 
 DE Is used For multidimensional real-valued functions but does Not use the gradient Of the problem 
 being optimized, which means DE does Not require For the optimization problem To be differentiable 
 As Is required by classic optimization methods such As gradient descent And quasi-newton methods. 
 DE can therefore also be used On optimization problems that are Not even continuous, are noisy, 
 change over time, etc.[1]
 
 DE optimizes a problem by maintaining a population Of candidate solutions And creating New candidate 
 solutions by combining existing ones according To its simple formulae, And Then keeping whichever 
 candidate solution has the best score Or fitness On the optimization problem at hand. In this way 
 the optimization problem Is treated As a black box that merely provides a measure Of quality given 
 a candidate solution And the gradient Is therefore Not needed.
 
 DE Is originally due To Storn And Price.[2][3] Books have been published On theoretical And practical 
 aspects Of Using DE In parallel computing, multiobjective optimization, constrained optimization, 
 And the books also contain surveys of application areas.[4][5][6][7] Excellent surveys on the 
 multi-faceted research aspects of DE can be found in journal articles Like.[8][9]
 </summary>
</member>
<member name="M:Microsoft.VisualBasic.MachineLearning.Darwinism.DifferentialEvolution.GetPopulation``1(Microsoft.VisualBasic.MachineLearning.Darwinism.DifferentialEvolution.New{``0},System.Int32,Microsoft.VisualBasic.Mathematical.IRandomSeeds)">
 <summary>
 Initialize population with individuals that have been initialized with uniform random noise
 uniform noise means random value inside your search space
 </summary>
 <param name="__new"></param>
 <param name="PopulationSize%"></param>
 <returns></returns>
 
</member>
<member name="M:Microsoft.VisualBasic.MachineLearning.Darwinism.DifferentialEvolution.Evolution``1(System.Func{``0,System.Double},Microsoft.VisualBasic.MachineLearning.Darwinism.DifferentialEvolution.New{``0},System.Int32,System.Double,System.Double,System.Double,System.Int32,System.Int32,System.Action{Microsoft.VisualBasic.MachineLearning.Darwinism.GAF.Helper.ListenerHelper.outPrint},System.Boolean,Microsoft.VisualBasic.Mathematical.IRandomSeeds)">
 <summary>
 
 </summary>
 <typeparam name="Individual"></typeparam>
 <param name="target"></param>
 <param name="[new]">How to creates a new <typeparamref name="Individual"/></param>
 <param name="N%">dimensionality of problem, means how many variables problem has.</param>
 <param name="threshold#"></param>
 <param name="maxIterations%"></param>
 <param name="F">differential weight [0,2]</param>
 <param name="CR">crossover probability [0,1]</param>
 <param name="PopulationSize%"></param>
 <returns></returns>
</member>
<member name="M:Microsoft.VisualBasic.MachineLearning.Darwinism.DifferentialEvolution.__subPopulationEvolute``1(``0[],System.Double,System.Int32,System.Double,System.Double,System.Int32,System.Action{Microsoft.VisualBasic.MachineLearning.Darwinism.GAF.Helper.ListenerHelper.outPrint},System.Func{``0,System.Double},Microsoft.VisualBasic.Mathematical.IRandomSeeds)">
 <summary>
 
 </summary>
 <typeparam name="Individual"></typeparam>
 <param name="population"></param>
 <param name="F#"></param>
 <param name="N%"></param>
 <param name="CR#"></param>
 <param name="bestFit#"></param>
 <param name="iterates%">i</param>
 <param name="iteratePrints"></param>
 <param name="fitnessFunction"></param>
 <returns></returns>
</member>
<member name="M:Microsoft.VisualBasic.MachineLearning.Darwinism.GAF.Fitness`1.Calculate(`0)">
 <summary>
 Assume that chromosome1 is better than chromosome2 <br/>
 fit1 = calculate(chromosome1) <br/>
 fit2 = calculate(chromosome2) <br/>
 So the following condition must be true <br/>
 fit1.compareTo(fit2) &lt;= 0 <br/>
 (假若是并行模式的之下，还要求这个函数是线程安全的)
 </summary>
</member>
<member name="F:Microsoft.VisualBasic.MachineLearning.Darwinism.GAF.GeneticAlgorithm`1.iterationListeners">
 <summary>
 listeners of genetic algorithm iterations (handle callback afterwards)
 </summary>
</member>
<member name="M:Microsoft.VisualBasic.MachineLearning.Darwinism.GAF.GeneticAlgorithm`1.#ctor(Microsoft.VisualBasic.MachineLearning.Darwinism.GAF.Population{`0},Microsoft.VisualBasic.MachineLearning.Darwinism.GAF.Fitness{`0},Microsoft.VisualBasic.Mathematical.IRandomSeeds)">
 <summary>
 
 </summary>
 <param name="population"></param>
 <param name="fitnessFunc">
 Calculates the fitness of the mutated chromesome in <paramref name="population"/>
 </param>
 <param name="seeds"></param>
</member>
<member name="M:Microsoft.VisualBasic.MachineLearning.Darwinism.GAF.GeneticAlgorithm`1.__iterate(System.Int32)">
 <summary>
 并行化过程之中的单个迭代
 </summary>
 <param name="i%"></param>
 <returns></returns>
</member>
<member name="P:Microsoft.VisualBasic.MachineLearning.Darwinism.GAF.GeneticAlgorithm`1.ParentChromosomesSurviveCount">
 <summary>
 Number of parental chromosomes, which survive (and move to new
 population)
 </summary>
 <returns></returns>
</member>
<member name="M:Microsoft.VisualBasic.MachineLearning.Darwinism.GAF.GeneticAlgorithm`1.Clear">
 <summary>
 Clear the internal cache
 </summary>
</member>
<member name="T:Microsoft.VisualBasic.MachineLearning.Darwinism.GAF.Helper.ChromosomesComparator`1">
 <summary>
 缓存的Key是染色体的ToString的计算值
 </summary>
 <typeparam name="C"></typeparam>
</member>
<member name="M:Microsoft.VisualBasic.MachineLearning.Darwinism.GAF.Helper.FitnessHelper.Calculate(System.Double[],System.Double[])">
 <summary>
 Implements Fitness(Of C, T).Calculate
 </summary>
 <param name="chromosome#"></param>
 <param name="target#"></param>
 <returns></returns>
</member>
<member name="M:Microsoft.VisualBasic.MachineLearning.Darwinism.GAF.Helper.GeneticHelper.Mutate(System.Double[]@,System.Random)">
 <summary>
 Returns clone of current chromosome, which is mutated a bit
 </summary>
 <param name="v#"></param>
 <param name="random"></param>
</member>
<member name="M:Microsoft.VisualBasic.MachineLearning.Darwinism.GAF.Helper.GeneticHelper.Mutate(System.Int32[]@,System.Random)">
 <summary>
 Returns clone of current chromosome, which is mutated a bit
 </summary>
 <param name="v%"></param>
 <param name="random"></param>
</member>
<member name="M:Microsoft.VisualBasic.MachineLearning.Darwinism.GAF.Helper.GeneticHelper.Crossover``1(System.Random,``0[]@,``0[]@)">
 <summary>
 Returns list of siblings 
 Siblings are actually new chromosomes, 
 created using any of crossover strategy
 </summary>
 <param name="random"></param>
 <param name="v1#"></param>
 <param name="v2#"></param>
</member>
<member name="M:Microsoft.VisualBasic.MachineLearning.Darwinism.GAF.Helper.GeneticHelper.InitialPopulation``1(``0,System.Int32,Microsoft.VisualBasic.MachineLearning.Darwinism.GAF.ParallelComputing{``0})">
 <summary>
 The simplest strategy for creating initial population <br/>
 in real life it could be more complex
 </summary>
</member>
<member name="M:Microsoft.VisualBasic.MachineLearning.Darwinism.GAF.Helper.ListenerHelper.AddDefaultListener``1(Microsoft.VisualBasic.MachineLearning.Darwinism.GAF.GeneticAlgorithm{``0}@,System.Action{Microsoft.VisualBasic.MachineLearning.Darwinism.GAF.Helper.ListenerHelper.outPrint},System.Double)">
 <summary>
 After each iteration Genetic algorithm notifies listener
 </summary>
 
</member>
<member name="P:Microsoft.VisualBasic.MachineLearning.Darwinism.GAF.Population`1.Parallel">
 <summary>
 是否使用并行模式在排序之前来计算出fitness
 </summary>
 <returns></returns>
</member>
<member name="M:Microsoft.VisualBasic.MachineLearning.Darwinism.GAF.Population`1.Add(`0)">
 <summary>
 Add chromosome
 </summary>
 <param name="chromosome"></param>
</member>
<member name="P:Microsoft.VisualBasic.MachineLearning.Darwinism.GAF.Population`1.Size">
 <summary>
 The number of chromosome elements in the inner list
 </summary>
 <returns></returns>
</member>
<member name="P:Microsoft.VisualBasic.MachineLearning.Darwinism.GAF.Population`1.Random(System.Random)">
 <summary>
 Gets random chromosome
 </summary>
 <returns></returns>
</member>
<member name="P:Microsoft.VisualBasic.MachineLearning.Darwinism.GAF.Population`1.Item(System.Int32)">
 <summary>
 Gets chromosome by index
 </summary>
 <param name="index%"></param>
 <returns></returns>
</member>
<member name="M:Microsoft.VisualBasic.MachineLearning.Darwinism.GAF.Population`1.GA_PLinq(Microsoft.VisualBasic.MachineLearning.Darwinism.GAF.GeneticAlgorithm{`0},Microsoft.VisualBasic.ComponentModel.DataSourceModel.NamedValue{`0}[])">
 <summary>
 使用PLinq进行并行计算
 </summary>
 <param name="GA"></param>
 <param name="source"></param>
 <returns></returns>
</member>
<member name="M:Microsoft.VisualBasic.MachineLearning.Darwinism.GAF.Population`1.SortPopulationByFitness(Microsoft.VisualBasic.MachineLearning.Darwinism.GAF.GeneticAlgorithm{`0},Microsoft.VisualBasic.MachineLearning.Darwinism.GAF.Helper.ChromosomesComparator{`0})">
 <summary>
 这里是ODEs参数估计的限速步骤
 </summary>
 <param name="GA"></param>
 <param name="comparator"></param>
</member>
<member name="M:Microsoft.VisualBasic.MachineLearning.Darwinism.GAF.Population`1.Trim(System.Int32)">
 <summary>
 shortening population till specific number
 </summary>
</member>
<member name="T:Microsoft.VisualBasic.MachineLearning.Darwinism.Models.Chromosome`1">
 <summary>
 In computer programming, genetic representation is a way of representing 
 solutions/individuals in evolutionary computation methods. Genetic 
 representation can encode appearance, behavior, physical qualities of 
 individuals. Designing a good genetic representation that is expressive 
 and evolvable is a hard problem in evolutionary computation. Difference 
 in genetic representations is one of the major criteria drawing a line 
 between known classes of evolutionary computation.

 Terminology often comes by analogy With natural genetics. The block Of 
 computer memory that represents one candidate solution Is called an individual. 
 The data In that block Is called a chromosome. Each chromosome consists Of 
 genes. The possible values Of a particular gene are called alleles. A 
 programmer may represent all the individuals Of a population Using binary 
 encoding, permutational encoding, encoding by tree, Or any one Of several 
 other representations.

 Genetic algorithms use linear binary representations. The most standard one 
 Is an array Of bits. Arrays Of other types And structures can be used In 
 essentially the same way. The main Property that makes these genetic 
 representations convenient Is that their parts are easily aligned due To 
 their fixed size. This facilitates simple crossover operation. Variable 
 length representations were also explored In Genetic algorithms, but crossover 
 implementation Is more complex In this Case.

 Evolution strategy uses linear real-valued representations, e.g. an array 
 Of real values. It uses mostly gaussian mutation And blending/averaging 
 crossover.

 Genetic programming(GP) pioneered tree-Like representations And developed 
 genetic operators suitable For such representations. Tree-Like representations 
 are used In GP To represent And evolve functional programs With desired 
 properties.

 Human-based genetic algorithm (HBGA) offers a way to avoid solving hard 
 representation problems by outsourcing all genetic operators to outside 
 agents, in this case, humans. The algorithm has no need for knowledge 
 of a particular fixed genetic representation as long as there are enough 
 external agents capable of handling those representations, allowing for 
 free-form And evolving genetic representations.
 </summary>
 <typeparam name="C"></typeparam>
</member>
<member name="M:Microsoft.VisualBasic.MachineLearning.Darwinism.Models.Chromosome`1.Crossover(`0)">
 <summary>
 In genetic algorithms, crossover is a genetic operator used to vary the programming 
 of a chromosome or chromosomes from one generation to the next. It is analogous to 
 reproduction and biological crossover, upon which genetic algorithms are based. 
 Cross over is a process of taking more than one parent solutions and producing a 
 child solution from them. There are methods for selection of the chromosomes.
 </summary>
 <param name="anotherChromosome"></param>
 <returns></returns>
</member>
<member name="M:Microsoft.VisualBasic.MachineLearning.Darwinism.Models.Chromosome`1.Mutate">
 <summary>
 Mutation is a genetic operator used to maintain genetic diversity from one generation 
 of a population of genetic algorithm chromosomes to the next. It is analogous to 
 biological mutation. Mutation alters one or more gene values in a chromosome from its 
 initial state. In mutation, the solution may change entirely from the previous solution. 
 Hence GA can come to better solution by using mutation. Mutation occurs during evolution 
 according to a user-definable mutation probability. This probability should be set low. 
 If it is set too high, the search will turn into a primitive random search.

 The classic example Of a mutation Operator involves a probability that an arbitrary bit 
 In a genetic sequence will be changed from its original state. A common method Of 
 implementing the mutation Operator involves generating a random variable For Each bit 
 In a sequence. This random variable tells whether Or Not a particular bit will be modified. 
 This mutation procedure, based On the biological point mutation, Is called Single point 
 mutation. Other types are inversion And floating point mutation. When the gene encoding 
 Is restrictive As In permutation problems, mutations are swaps, inversions, And scrambles.

 The purpose Of mutation In GAs Is preserving And introducing diversity. Mutation should 
 allow the algorithm To avoid local minima by preventing the population Of chromosomes 
 from becoming too similar To Each other, thus slowing Or even stopping evolution. This 
 reasoning also explains the fact that most GA systems avoid only taking the fittest Of 
 the population In generating the Next but rather a random (Or semi-random) selection 
 With a weighting toward those that are fitter.
 </summary>
 <returns></returns>
</member>
<member name="M:Microsoft.VisualBasic.MachineLearning.Darwinism.Models.FitnessPool`2.Fitness(`0)">
 <summary>
 This function tells how well given individual performs at given problem.
 </summary>
 <param name="[in]"></param>
 <returns></returns>
</member>
<member name="T:Microsoft.VisualBasic.MachineLearning.QLearning.Action">
 <summary>
 One specific environment state have some possible actions,
 but there is just one best action on the current environment state based on the accumulate q-values
 </summary>
</member>
<member name="P:Microsoft.VisualBasic.MachineLearning.QLearning.Action.EnvirState">
 <summary>
 The environment variables state as inputs for the machine.
 </summary>
 <returns></returns>
</member>
<member name="P:Microsoft.VisualBasic.MachineLearning.QLearning.Action.Qvalues">
 <summary>
 Actions for the current state.
 </summary>
 <returns></returns>
</member>
<member name="M:Microsoft.VisualBasic.MachineLearning.QLearning.Action.ToString">
 <summary>
 Environment -> actions' Q-values
 </summary>
 <returns></returns>
</member>
<member name="T:Microsoft.VisualBasic.MachineLearning.QLearning.DataModel.QModel">
 <summary>
 Data model of the <see cref="T:Microsoft.VisualBasic.MachineLearning.QLearning.QTable`1"/>, you can using this object to stores the trained QL_AI into a file.
 </summary>
</member>
<member name="T:Microsoft.VisualBasic.MachineLearning.QLearning.DataModel.IndexCurve">
 <summary>
 属性是时间
 </summary>
</member>
<member name="T:Microsoft.VisualBasic.MachineLearning.QLearning.QLearning`1">
 <summary>
 Q Learning sample class <br/>
 <b>The goal of this code sample is for the character @ to reach the goal area G</b> <br/>
 compile using "javac QLearning.java" <br/>
 test using "java QLearning" <br/>
 
 @author A.Liapis (Original author), A. Hartzen (2013 modifications) 
 </summary>
</member>
<member name="P:Microsoft.VisualBasic.MachineLearning.QLearning.QLearning`1.ActionRange">
 <summary>
 The size of the <see cref="T:Microsoft.VisualBasic.MachineLearning.QLearning.QTable`1"/>
 </summary>
 <returns></returns>
</member>
<member name="M:Microsoft.VisualBasic.MachineLearning.QLearning.QLearning`1.__run(System.Int32)">
 <summary>
 Takes a action for the agent.
 </summary>
 <param name="i">Iteration counts.</param>
</member>
<member name="M:Microsoft.VisualBasic.MachineLearning.QLearning.QLearning`1.__reset(System.Int32)">
 <summary>
 If the <see cref="P:Microsoft.VisualBasic.MachineLearning.QLearning.QLearning`1.GoalReached"/> then reset and continute learning.
 </summary>
 <param name="i">机器学习的当前的迭代次数</param>
</member>
<member name="P:Microsoft.VisualBasic.MachineLearning.QLearning.QLearning`1.GoalRewards">
 <summary>
 目标达成所得到的奖励
 </summary>
 <returns></returns>
</member>
<member name="P:Microsoft.VisualBasic.MachineLearning.QLearning.QLearning`1.GoalPenalty">
 <summary>
 目标没有达成的罚分
 </summary>
 <returns></returns>
</member>
<member name="M:Microsoft.VisualBasic.MachineLearning.QLearning.QLearning`1.__finishLearn">
 <summary>
 You can save you q table by overrides at here.
 </summary>
</member>
<member name="T:Microsoft.VisualBasic.MachineLearning.QLearning.QState`1">
 <summary>
 
 </summary>
 <typeparam name="T">Status object</typeparam>
</member>
<member name="P:Microsoft.VisualBasic.MachineLearning.QLearning.QState`1.State">
 <summary>
 假若操作不会涉及到数据修改，请使用这个属性来减少性能的损失，<see cref="P:Microsoft.VisualBasic.MachineLearning.QLearning.QState`1.Current"/>属性返回的值和本属性是一样的，
 只不过<see cref="P:Microsoft.VisualBasic.MachineLearning.QLearning.QState`1.Current"/>属性是从<see cref="M:System.ICloneable.Clone"/>方法得到的数据，所以性能方面会有损失
 </summary>
 <returns></returns>
</member>
<member name="P:Microsoft.VisualBasic.MachineLearning.QLearning.QState`1.Current">
 <summary>
 map before the action is taken, clone object: <see cref="M:System.ICloneable.Clone"/>
 </summary>
 <returns></returns>
</member>
<member name="M:Microsoft.VisualBasic.MachineLearning.QLearning.QState`1.GetNextState(System.Int32)">
 <summary>
 Gets the <see cref="P:Microsoft.VisualBasic.MachineLearning.QLearning.QState`1.Current"/> states.
 Returns the map state which results from an initial map state after an
 action is applied. In case the action is invalid, the returned map is the
 same as the initial one (no move). </summary>
 <param name="action"> taken by the avatar ('@') </param>
 <returns> resulting map after the action is taken </returns>
</member>
<member name="T:Microsoft.VisualBasic.MachineLearning.QLearning.QTable`1">
 <summary>
 The heart of the Q-learning algorithm, the QTable contains the table
 which maps states, actions and their Q values. This class has elaborate
 documentation, and should be the focus of the students' body of work
 for the purposes of this tutorial.

 @author A.Liapis (Original author), A. Hartzen (2013 modifications); xie.guigang@gcmodeller.org (2016 modifications)
 </summary>
</member>
<member name="F:Microsoft.VisualBasic.MachineLearning.QLearning.QTable`1.__randomGenerator">
 <summary>
 for creating random numbers
 </summary>
</member>
<member name="P:Microsoft.VisualBasic.MachineLearning.QLearning.QTable`1.Table">
 <summary>
 the table variable stores the Q-table, where the state is saved
 directly as the actual map. Each map state has an array of Q values
 for all the actions available for that state.
 </summary>
</member>
<member name="P:Microsoft.VisualBasic.MachineLearning.QLearning.QTable`1.ActionRange">
 <summary>
 the actionRange variable determines the number of actions available
 at any map state, and therefore the number of Q values in each entry
 of the Q-table.
 </summary>
</member>
<member name="P:Microsoft.VisualBasic.MachineLearning.QLearning.QTable`1.ExplorationChance">
 <summary>
 for e-greedy Q-learning, when taking an action a random number is
 checked against the explorationChance variable: if the number is
 below the explorationChance, then exploration takes place picking
 an action at random. Note that the explorationChance is not a final
 because it is customary that the exploration chance changes as the
 training goes on.
 </summary>
</member>
<member name="P:Microsoft.VisualBasic.MachineLearning.QLearning.QTable`1.GammaValue">
 <summary>
 the discount factor is saved as the gammaValue variable. The
 discount factor determines the importance of future rewards.
 If the gammaValue is 0 then the AI will only consider immediate
 rewards, while with a gammaValue near 1 (but below 1) the AI will
 try to maximize the long-term reward even if it is many moves away.
 </summary>
</member>
<member name="P:Microsoft.VisualBasic.MachineLearning.QLearning.QTable`1.LearningRate">
 <summary>
 the learningRate determines how new information affects accumulated
 information from previous instances. If the learningRate is 1, then
 the new information completely overrides any previous information.
 Note that the learningRate is not a final because it is
 customary that the learningRate changes as the
 training goes on.
 </summary>
</member>
<member name="F:Microsoft.VisualBasic.MachineLearning.QLearning.QTable`1._prevState">
 <summary>
 Since in Q-learning the updates to the Q values are made ONE STEP
 LATE, the state of the world when the action resulting in the reward
 was made must be stored.
 </summary>
</member>
<member name="F:Microsoft.VisualBasic.MachineLearning.QLearning.QTable`1._prevAction">
 <summary>
 Since in Q-learning the updates to the Q values are made ONE STEP
 LATE, the index of the action which resulted in the reward must be
 stored.
 </summary>
</member>
<member name="M:Microsoft.VisualBasic.MachineLearning.QLearning.QTable`1.#ctor(System.Int32)">
 <summary>
 Q table constructor, initiates variables. </summary>
 <param name="actionRange"> number of actions available at any map state </param>
</member>
<member name="M:Microsoft.VisualBasic.MachineLearning.QLearning.QTable`1.NextAction(`0)">
 <summary>
 For this example, the getNextAction function uses an e-greedy
 approach, having exploration happen if the exploration chance
 is rolled.
 ( **** 请注意，这个函数所返回的值为最佳选择的Index编号，所以可能还需要进行一些转换 **** )
 </summary>
 <param name="map"> current map (state) </param>
 <returns> the action to be taken by the calling program </returns>
</member>
<member name="M:Microsoft.VisualBasic.MachineLearning.QLearning.QTable`1.__getBestAction(`0)">
 <summary>
 The getBestAction function uses a greedy approach for finding
 the best action to take. Note that if all Q values for the current
 state are equal (such as all 0 if the state has never been visited
 before), then getBestAction will always choose the same action.
 If such an action is invalid, this may lead to a deadlock as the
 map state never changes: for situations like these, exploration
 can get the algorithm out of this deadlock.
 </summary>
 <param name="map"> current map (state) </param>
 <returns> the action with the highest Q value </returns>
</member>
<member name="M:Microsoft.VisualBasic.MachineLearning.QLearning.QTable`1.__explore">
 <summary>
 The explore function is called for e-greedy algorithms.
 It can choose an action at random from all available,
 or can put more weight towards actions that have not been taken
 as often as the others (most unknown).
 </summary>
 <returns> index of action to take </returns>
 <remarks>在这里得到可能的下一步的动作的在动作列表里面编号值， Index</remarks>
</member>
<member name="M:Microsoft.VisualBasic.MachineLearning.QLearning.QTable`1.UpdateQvalue(System.Int32,`0)">
 <summary>
 The updateQvalue is the heart of the Q-learning algorithm. Based on
 the reward gained by taking the action prevAction while being in the
 state prevState, the updateQvalue must update the Q value of that
 {prevState, prevAction} entry in the Q table. In order to do that,
 the Q value of the best action of the current map state must also
 be calculated.
 </summary>
 <param name="reward"> at the current map state </param>
 <param name="map"> current map state (for finding the best action of the
 current map state) </param>
</member>
<member name="M:Microsoft.VisualBasic.MachineLearning.QLearning.QTable`1.__getMapString(`0)">
 <summary>
 This helper function is used for entering the map state into the
 HashMap </summary>
 <param name="map"> </param>
 <returns> String used as a key for the HashMap </returns>
</member>
<member name="M:Microsoft.VisualBasic.MachineLearning.QLearning.QTable`1.__getActionsQValues(`0)">
 <summary>
 The getActionsQValues function returns an array of Q values for
 all the actions available at any state. Note that if the current
 map state does not already exist in the Q table (never visited
 before), then it is initiated with Q values of 0 for all of the
 available actions.
 </summary>
 <param name="map"> current map (state) </param>
 <returns> an array of Q values for all the actions available at any state </returns>
</member>
<member name="M:Microsoft.VisualBasic.MachineLearning.QLearning.QTable`1.GetValues(`0)">
 <summary>
 Helper function to find the Q-values of a given map state.
 </summary>
 <param name="map"> current map (state) </param>
 <returns> the Q-values stored of the Qtable entry of the map state, otherwise null if it is not found </returns>
</member>
</members>
</doc>
